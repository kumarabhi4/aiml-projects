# Project details and summary

## 1. Supervised learning techniques:

- Problem Statement: Medical research university X is undergoing a deep research on patients with certain conditions. University has an internal AI team.
Due to conidentiality the patient’s details and the conditions are masked by the client by providing different datasets to the AI team for
developing a AIML model which can predict the condition of the patient depending on the received test results.

- Tools used: Python, SQL, NumPy, Pandas, Seaborn, Scikit Learn, Matplotlib, Jupyter Notebook, Google Colab, GitHub

- Objective: To Demonstrate the ability to fetch, process and leverage data to generate useful predictions by training Supervised Learning algorithms.
  - Clean the data 
  - Visualize the data
  - Find any hidden correlation among data
  - Predict the codition of the patient
 
  

## 2. Unsupervised Learning Techniques:

- Problem Statement: The data concerns city-cycle fuel consumption in miles per gallon to be predicted in terms of 3 multivalued discrete and 5
continuous attributes.

- Tools used: Python, NumPy, Pandas, Seaborn, SciPy, Scikit Learn, K-means Clustering Algorithm, Matplotlib, Jupyter Notebook, Google Colab, GitHub

- Objective: To understand K-means Clustering by applying on the Car Dataset to segment the cars into various categories.
  - Clean the data
  - Visualize the data
  - Understand the data in terms of Std deviation, mean, mode etc.
  - Find any hidden correlation among data
  - Predict fuel consumption
  
  
## 3. Featurisation and Model Tuning:

- Problem Statement: A complex modern semiconductor manufacturing process is normally under constant surveillance via the monitoring of
signals/variables collected from sensors and or process measurement points. However, not all of these signals are equally valuable in a
speciic monitoring system. The measured signals contain a combination of useful information, irrelevant information as well as noise.
Engineers typically have a much larger number of signals than are actually required. If we consider each type of signal as a feature, then
feature selection may be applied to identify the most relevant signals. The Process Engineers may then use these signals to determine key
factors contributing to yield excursions downstream in the process. This will enable an increase in process throughput, decreased time to
learning and reduce the per unit production costs. These signals can be used as features to predict the yield type. And by analysing and
trying out different combinations of features, essential signals that are impacting the yield type can be identiied.

- Tools used: Python, NumPy, Pandas, Seaborn, Scikit Learn, Boosting, Bagging, Matplotlib, Jupyter Notebook, Google Colab, GitHub

- Objective: : Build a classifier to predict the Pass/Fail yield of a particular process entity and analyse whether all the features are required to build the model or not.
  - Clean the data
  - Visualize the data
  - Understand the data 
  - Select most suitable features for model building
  - Find any hidden correlation among data
  - predict the Pass/Fail yield of a particular process
  


## 4. Computer Vision:

- Problem Statement: Company X owns a movie application and repository that caters to movie streaming to millions of users on a subscription basis. 
The company wants to automate the process of cast and crew information in each scene from a movie such that when a user pauses on the movie and 
clicks on the cast information button, the app will show details of the actor in the scene. The company has in-house computer vision and multimedia 
experts who need to detect faces from screenshots of the movie scene.

- Tools used: Python, NumPy, CV2, Pandas, Seaborn, Scikit Learn, Tensorflow, Keras, Matplotlib, Jupyter Notebook, Google Colab, GitHub

- Objective: To build a face detection system, to create an image dataset to be used by the AI team to build image classifier data and to build a face recognition system
  - Clean the data and preprocessing
  - Visualize the data
  - Image masking
  - Understand different neuron layers of prebuild algorithms
  - Detect and recognise facing from the image
  
  
  
  
## 5. Neural Networks:

- Problem Statement:  A communications equipment manufacturing company has a product which is responsible for emitting informative signals.
Company wants to build a machine learning model which can help the company to predict the equipment’s signal quality using various
parameters. 

- Tools used: Python, NumPy, CV2, Pandas, Seaborn, Scikit Learn, Tensorflow, Keras, Matplotlib, Jupyter Notebook, Google Colab, GitHub

- Objective: To build a classiier which can use the given parameters to determine the signal strength or quality.
  - Clean the data
  - Visualize the data
  - Understand the data 
  - Select most suitable features for model building
  - Find any hidden correlation among data
  - predict the Pass/Fail yield of a particular process
  
  
## 6. Natural Langues Processing:

- Problem Statement:  The prices of the stocks of companies listed under a global exchange are influenced by a variety of factors, with the company's 
financial performance, innovations and collaborations, and market sentiment being factors that play a significant role. News and media reports can 
rapidly affect investor perceptions and, consequently, stock prices in the highly competitive financial industry. With the sheer volume of news and 
opinions from a wide variety of sources, investors and financial analysts often struggle to stay updated and accurately interpret its impact on the market. 
As a result, investment firms need sophisticated tools to analyze market sentiment and integrate this information into their investment strategies. 

- Tools used: Python, NumPy, NLTK, Pandas, Seaborn, Scikit Learn, Tensorflow, Keras, Matplotlib, Jupyter Notebook, Google Colab, GitHub, Hugging Face, Word Embedding

- Objective: To develop an AI-driven sentiment analysis system that will automatically process and analyze news articles to gauge market sentiment, 
and summarizing the news at a weekly level to enhance the accuracy of their stock price predictions and optimize investment strategies.
  - Text cleaning and preprocessing
  - Visualize some of the text
  - Word Embedding
  - Understand the data 
  - Find any hidden correlation among texts
  - predict the sentiments in the news headlines
  
  
## 7. Capstone Project- Predicting Pnumonia on medical images.
- Problem Statement:  Computer vision can be used in health care for identifying diseases. In Pneumonia detection we need to detect Inflammation
of the lungs. In this challenge, you’re required to build an algorithm to detect a visual signal for pneumonia in medical
images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. 

- Tools used: Python, NumPy, CV2, Pandas, Seaborn, Scikit Learn, Tensorflow, Keras, Matplotlib, Jupyter Notebook, Google Colab, GitHub

- Objective: Design a DL based algorithm for detecting pneumonia.
  - Clean the data and preprocessing
  - Visualize the data
  - Image masking and bouding boxes identification
  - Fine tunning CNN models and understanding different layers of Convolutional Network
  - Pickle the mode for future use
